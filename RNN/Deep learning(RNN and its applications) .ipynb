{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a104b6aa",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network and  its applications  =>\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a type of neural network designed to process sequential data by maintaining an internal state, or memory, to capture information from previous inputs. They are particularly useful when dealing with sequential and temporal data, as they can learn patterns and dependencies over time.\n",
    "\n",
    "Here are some key reasons why RNNs are used:\n",
    "\n",
    "(1). Sequential Data Processing: RNNs excel at processing sequential data, where the order of elements matters, such as time series data, natural language processing (NLP), speech recognition, and handwriting recognition. They can capture dependencies between different elements in the sequence.\n",
    "    \n",
    "\n",
    "(2). Variable-Length Inputs: RNNs can handle variable-length inputs and produce corresponding outputs of the same sequence length. This flexibility is valuable in applications where inputs or outputs have varying lengths, such as text generation or speech synthesis.\n",
    "    \n",
    "\n",
    "(3). Memory and Contextual Information: RNNs maintain internal memory to store information about past inputs, allowing them to retain context and information from earlier elements in the sequence. This memory enables the network to make decisions based on previous inputs and their relationships.\n",
    "    \n",
    "\n",
    "(4). Time Series Analysis: RNNs are commonly used for analyzing time series data, such as financial data, weather data, and physiological signals. By considering the temporal nature of the data, RNNs can model trends, dependencies, and patterns over time.\n",
    "    \n",
    "\n",
    "(5). Natural Language Processing (NLP): RNNs have proven to be highly effective in NLP tasks, including language modeling, machine translation, sentiment analysis, text classification, and named entity recognition. They can capture the semantic and syntactic structure of language by processing words or characters sequentially.\n",
    "    \n",
    "\n",
    "(6). Speech Recognition and Generation: RNNs are extensively used in speech recognition systems to convert spoken language into written text. They can also be employed in speech synthesis applications, generating human-like speech from written text.\n",
    "    \n",
    "\n",
    "(7). Image Captioning: RNNs can be combined with convolutional neural networks (CNNs) to generate descriptive captions for images. The CNN processes the image, and its output is fed into the RNN, which generates the corresponding caption.\n",
    "    \n",
    "\n",
    "(8). Video Analysis: RNNs are applicable to video analysis tasks, such as action recognition, video captioning, and video summarization. By considering the temporal dependencies between frames, RNNs can understand the temporal structure of videos.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8860b5c",
   "metadata": {},
   "source": [
    "# What is Sequential Data ? \n",
    "\n",
    "Sequential data refers to a type of data where the order or sequence of elements carries significance and affects the interpretation or analysis of the data. In sequential data, the position or arrangement of elements conveys information or patterns that need to be captured and understood.\n",
    "\n",
    "Real-life examples of sequential data include:\n",
    "\n",
    "(1). Time Series Data: Time series data is a classic example of sequential data. It involves a sequence of data points recorded over time, where each point represents a measurement or observation taken at a specific time. Examples include stock prices, temperature recordings, heart rate monitoring, and daily sales data. The order of the data points is crucial for understanding trends, seasonality, and patterns over time.\n",
    "    \n",
    "\n",
    "(2). Natural Language Text: Textual data, such as sentences or paragraphs in natural language, is inherently sequential. The order of words and characters carries meaning and determines the semantics and syntax of the text. Language models, machine translation, sentiment analysis, and text generation tasks all rely on capturing the sequential structure of text.\n",
    "    \n",
    "\n",
    "(3). Music and Audio Signals: Musical compositions and audio signals are sequential in nature. Musical notes played over time form a sequence that needs to be captured to understand melodies, rhythms, and harmonies. Similarly, audio signals like speech, music recordings, or environmental sounds can be represented as a sequence of samples over time.\n",
    "    \n",
    "\n",
    "(4). DNA Sequences: In genetics, DNA sequences represent the order of nucleotides (adenine, thymine, cytosine, and guanine) that make up an organism's genetic material. Analyzing and understanding DNA sequences is crucial in various biological applications, including genetic research, disease diagnosis, and evolutionary studies.\n",
    "    \n",
    "\n",
    "(5). User Behavior Data: Sequential data can also be found in user behavior logs, such as web clickstreams or transaction histories. The order of actions taken by users provides insights into their browsing patterns, preferences, or purchasing behaviors. Analyzing this sequential data can help optimize recommendation systems, personalize user experiences, or detect anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbada6b",
   "metadata": {},
   "source": [
    "# RNN  forward propagation with time ? \n",
    "\n",
    "In an RNN, the input sequence is processed step-by-step over time, and at each time step, the network produces an output and updates its hidden state. The hidden state serves as the memory that retains information from previous time steps, allowing the network to capture dependencies and patterns over time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "851b4219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [0.08395493 0.34156022]\n",
      "Output: [ 0.99532269 -1.4258815 ]\n",
      "Output: [ 0.7527349  -2.31889535]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the RNN parameters\n",
    "input_size = 4  # Dimensionality of the input at each time step\n",
    "hidden_size = 3  # Dimensionality of the hidden state\n",
    "output_size = 2  # Dimensionality of the output at each time step\n",
    "\n",
    "# Define the input sequence\n",
    "sequence = np.array([[1, 2, 3, 4],\n",
    "                     [5, 6, 7, 8],\n",
    "                     [9, 10, 11, 12]])\n",
    "\n",
    "# Initialize the RNN weights\n",
    "Wxh = np.random.randn(hidden_size, input_size)  # Weight matrix for input-to-hidden connections\n",
    "Whh = np.random.randn(hidden_size, hidden_size)  # Weight matrix for hidden-to-hidden connections\n",
    "Why = np.random.randn(output_size, hidden_size)  # Weight matrix for hidden-to-output connections\n",
    "bh = np.zeros((hidden_size, 1))  # Bias for hidden state\n",
    "by = np.zeros((output_size, 1))  # Bias for output\n",
    "\n",
    "# Initialize the hidden state\n",
    "h_prev = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Perform forward propagation through time\n",
    "for x in sequence:\n",
    "    # Convert input to column vector\n",
    "    x = x.reshape(-1, 1)\n",
    "    \n",
    "    # Calculate the hidden state\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h_prev) + bh)\n",
    "    \n",
    "    # Calculate the output\n",
    "    y = np.dot(Why, h) + by\n",
    "    \n",
    "    # Print the output at each time step\n",
    "    print(\"Output:\", y.ravel())\n",
    "    \n",
    "    # Update the hidden state for the next time step\n",
    "    h_prev = h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c40598",
   "metadata": {},
   "source": [
    "# In this example, we have a simple RNN with an input size of 4, a hidden size of 3, and an output size of 2. The input sequence consists of three time steps, where each time step has a 4-dimensional input vector.\n",
    "\n",
    "The forward propagation through time is performed using a loop that iterates over each time step in the sequence. At each time step, the input vector is processed by the RNN to compute the hidden state, which is then used to calculate the output. The hidden state from the previous time step is also updated and passed on to the next time step.\n",
    "\n",
    "The weights (Wxh, Whh, Why) and biases (bh, by) of the RNN are randomly initialized. The tanh activation function is used for the hidden state calculation to introduce non-linearity.\n",
    "\n",
    "The code prints the output at each time step, demonstrating how the RNN processes the input sequence over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2320ad22",
   "metadata": {},
   "source": [
    "# Back Propagation in RNN ? \n",
    "\n",
    "Backpropagation in recurrent neural networks (RNNs) is a technique used to train the network by propagating the error gradients from the output back to the input layer. It allows the network to learn and adjust its weights based on the error between the predicted output and the desired output. Here's a step-by-step explanation of how backpropagation works in RNNs:\n",
    "\n",
    "(1). Forward Pass: During the forward pass, the input data is fed into the RNN layer, and the activations are calculated at each time step. The activations are usually computed using a non-linear activation function such as the hyperbolic tangent (tanh) or sigmoid function.\n",
    "\n",
    "\n",
    "(2). Loss Calculation: After the forward pass, the output of the RNN layer is compared with the desired output, and a loss function is computed to measure the difference between them. Common loss functions for different tasks include mean squared error (MSE) for regression and categorical cross-entropy for classification.\n",
    "\n",
    "\n",
    "(3). Backward Pass: In the backward pass, the gradients of the loss function with respect to the network parameters (weights and biases) are calculated. The gradient is a measure of how much the loss changes with respect to each parameter. The goal is to adjust the parameters in a way that minimizes the loss.\n",
    "\n",
    "\n",
    "(4). Backpropagation Through Time (BPTT): Since RNNs have recurrent connections, backpropagation is performed over time steps as well. BPTT unfolds the recurrent connections into multiple time steps, creating a computational graph that allows the gradients to flow through time. The gradients are calculated by applying the chain rule of calculus to propagate the gradients backwards from the output to the input layer.\n",
    "\n",
    "\n",
    "(5). Parameter Update: Once the gradients are calculated, the parameters of the RNN are updated using an optimization algorithm such as stochastic gradient descent (SGD) or Adam. The update rule typically involves multiplying the gradients by a learning rate and subtracting them from the current parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "276281e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=1>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the RNN model\n",
    "rnn = tf.keras.layers.SimpleRNN(units=64)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# Generate dummy data\n",
    "input_data = tf.random.normal(shape=(32, 10, 32))\n",
    "target_data = tf.random.normal(shape=(32, 64))  # Adjust the shape to match predictions\n",
    "\n",
    "# Perform forward pass\n",
    "with tf.GradientTape() as tape:\n",
    "    predictions = rnn(input_data)\n",
    "    loss_value = loss_fn(target_data, predictions)\n",
    "\n",
    "# Perform backward pass\n",
    "gradients = tape.gradient(loss_value, rnn.trainable_variables)\n",
    "\n",
    "# Update parameters\n",
    "optimizer.apply_gradients(zip(gradients, rnn.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "242012ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336ccc29",
   "metadata": {},
   "source": [
    "# What are problems in Simple RNN ?  \n",
    "\n",
    "\n",
    "Simple RNNs (Recurrent Neural Networks) have been widely used for sequence modeling tasks. However, they suffer from several limitations, which can make them challenging to train effectively. Here are some of the problems associated with Simple RNNs:\n",
    "\n",
    "(1). Vanishing/Exploding Gradients: One of the major issues with Simple RNNs is the vanishing or exploding gradient problem. During backpropagation through time, gradients can either shrink exponentially or grow uncontrollably, leading to difficulties in training the network. This problem arises because of the repetitive multiplicative nature of the recurrent connections, which can cause the gradients to quickly diminish or explode as they propagate through time steps.\n",
    "\n",
    "\n",
    "(2). Short-Term Memory: Simple RNNs have a limited ability to capture long-term dependencies in sequential data. They tend to perform well on short sequences but struggle to retain information over longer sequences. As the network is trained over time, the influence of early time steps on later ones diminishes due to the vanishing gradient problem.\n",
    "\n",
    "\n",
    "(3). Lack of Contextual Understanding: Simple RNNs treat each time step independently and do not consider the context from previous time steps when making predictions. As a result, they may fail to understand the overall context or dependencies in the sequence, limiting their performance on tasks that require contextual understanding.\n",
    "\n",
    "\n",
    "(4). Difficulty in Capturing Long-Term Dependencies: Simple RNNs face challenges in capturing long-term dependencies, which are crucial for many sequence-related tasks. When there is a time gap between relevant information in the sequence, the gradients may become too small to propagate the error effectively, resulting in the network's inability to learn the dependencies.\n",
    "\n",
    "\n",
    "To overcome these limitations, more advanced RNN architectures have been introduced, such as LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit). These architectures include additional mechanisms, such as memory cells and gating mechanisms, that enable them to capture long-term dependencies more effectively and alleviate the vanishing/exploding gradient problem.\n",
    "\n",
    "LSTM introduces memory cells and gates that regulate the flow of information within the network, allowing it to selectively retain or forget information over time. GRU simplifies the LSTM architecture by combining the forget and input gates into a single \"update gate,\" making it computationally less expensive while still providing effective long-term dependency modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c13b80",
   "metadata": {},
   "source": [
    "# Complete Understanding of LSTM Networks => \n",
    "\n",
    "https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da40c2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
