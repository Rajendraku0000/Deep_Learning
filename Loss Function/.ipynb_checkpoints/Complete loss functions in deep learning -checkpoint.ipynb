{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e824a50e",
   "metadata": {},
   "source": [
    "# Complete Loss Functions ==> \n",
    "\n",
    "Loss functions are an integral part of deep learning models as they quantify the discrepancy between predicted and actual values.\n",
    "\n",
    "(1). Mean Squared Error (MSE) Loss:\n",
    "\n",
    "Definition: Calculates the average squared difference between the predicted and target values.\n",
    "    \n",
    "Formula: MSE = (1/n) * Σ(y_pred - y_true)^2, where n is the number of samples.\n",
    "    \n",
    "Suitable for: Regression tasks where the target values are continuous.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "(2). Binary Cross-Entropy Loss:\n",
    "\n",
    "Definition: Measures the dissimilarity between two probability distributions (binary classification).\n",
    "    \n",
    "Formula: BCE = -Σ(y_true * log(y_pred) + (1 - y_true) * log(1 - y_pred)), where y_true and y_pred are the true and predicted probabilities, respectively.\n",
    "    \n",
    "Suitable for: Binary classification problems.\n",
    "    \n",
    "    \n",
    "(3). Categorical Cross-Entropy Loss:\n",
    "\n",
    "Definition: Calculates the cross-entropy loss for multi-class classification problems.\n",
    "    \n",
    "Formula: CCE = -Σ(y_true * log(y_pred)), where y_true and y_pred are the true and predicted class probabilities, respectively.\n",
    "    \n",
    "Suitable for: Multi-class classification tasks.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "(4). Sparse Categorical Cross-Entropy Loss:\n",
    "\n",
    "Definition: Similar to categorical cross-entropy, but the true labels are given as integers instead of one-hot encoded vectors.\n",
    "    \n",
    "Formula: SCCE = -Σ(log(y_pred[true_label])), where true_label represents the integer class label.\n",
    "    \n",
    "Suitable for: Multi-class classification tasks with integer-encoded labels.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "(5).  Kullback-Leibler Divergence (KL Divergence) Loss:\n",
    "\n",
    "Definition: Measures the difference between two probability distributions.\n",
    "    \n",
    "Formula: KL = Σ(y_true * log(y_true / y_pred)), where y_true and y_pred are the true and predicted probability distributions, respectively.\n",
    "    \n",
    "Suitable for: Tasks involving probabilistic models or when matching two distributions.\n",
    "    \n",
    "    \n",
    "(6).  Huber Loss:\n",
    "\n",
    "Definition: Combines the characteristics of both MSE and MAE (Mean Absolute Error) to provide robustness against outliers.\n",
    "    \n",
    "Formula: Huber = (1/n) * Σ(L), where L = 0.5 * (y_true - y_pred)^2 if |y_true - y_pred| <= δ, and L = δ * |y_true - y_pred| - 0.5 * δ^2 otherwise, where δ is a threshold value.\n",
    "    \n",
    "Suitable for: Regression tasks with outliers in the target data.\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "(7).  Mean Absolute Error (MAE) Loss:\n",
    "\n",
    "Definition: Calculates the average absolute difference between the predicted and target values.\n",
    "    \n",
    "Formula: MAE = (1/n) * Σ|y_pred - y_true|, where n is the number of samples.\n",
    "    \n",
    "Suitable for: Regression tasks where the target values are continuous.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "(8). Hinge Loss (SVM Loss):\n",
    "\n",
    "Definition: Used in support vector machines (SVMs) and measures the margin of classification error.\n",
    "    \n",
    "Formula: Hinge = Σmax(0, 1 - y_true * y_pred), where y_true and y_pred are the true and predicted class labels, respectively.\n",
    "    \n",
    "Suitable for: Binary classification tasks where SVMs or linear classifiers are employed.\n",
    "    \n",
    "    \n",
    "    \n",
    "(9). Sigmoid Cross-Entropy Loss:\n",
    "\n",
    "Definition: Combines the sigmoid activation function and binary cross-entropy loss.\n",
    "    \n",
    "Formula: Sigmoid_CE = -Σ(y_true * log(sigmoid(y_pred)) + (1 - y_true) * log(1 - sigmoid(y_pred))), where y_true and y_pred are the true labels and predicted logits, respectively.\n",
    "    \n",
    "Suitable for: Binary classification problems when using a sigmoid activation function.\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "(10). Focal Loss:\n",
    "\n",
    "Definition: A variant of cross-entropy loss that addresses class imbalance by downweighting well-classified examples.\n",
    "    \n",
    "Formula: FL = -Σ((1 - y_pred)^γ * y_true * log(y_pred)), where y_true and y_pred are the true labels and predicted probabilities, respectively, and γ is the focusing parameter.\n",
    "    \n",
    "Suitable for: Imbalanced classification tasks where the majority class dominates.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(11). Wasserstein Loss (Earth Mover's Distance):\n",
    "\n",
    "Definition: Used in generative adversarial networks (GANs) to measure the dissimilarity between the generated and real data distributions.\n",
    "\n",
    "Formula: W = Σ(D(G(z)) - D(x)), where D represents the discriminator, G represents the generator, z represents the input noise, and x represents real data samples.\n",
    "\n",
    "Suitable for: Training GANs and learning the generator and discriminator models.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(12).  Triplet Loss:\n",
    "\n",
    "Definition: Used in metric learning tasks to encourage the embedding of similar samples to be closer while pushing dissimilar samples farther apart.\n",
    "\n",
    "Formula: TL = Σ(max(0, ||f(a) - f(p)|| - ||f(a) - f(n)|| + α)), where f represents the embedding function, a is an anchor sample, p is a positive sample, n is a negative sample, and α is a margin parameter.\n",
    "\n",
    "Suitable for: Face recognition, image retrieval, and other tasks where learning similarity metrics is important.\n",
    "\n",
    "\n",
    "(13). Dice Loss (Sørensen-Dice Loss):\n",
    "\n",
    "Definition: Evaluates the overlap between predicted and true regions, often used in segmentation tasks.\n",
    "\n",
    "Formula: Dice = 1 - (2 * Σ(y_pred * y_true) + ε) / (Σy_pred + Σy_true + ε), where y_true and y_pred are the true and predicted binary masks, respectively, and ε is a small constant to avoid division by zero.\n",
    "\n",
    "Suitable for: Image segmentation problems.\n",
    "\n",
    "\n",
    "\n",
    "(14). Contrastive Loss:\n",
    "\n",
    "Definition: Encourages similar samples to have similar embeddings while pushing dissimilar samples apart in a learned embedding space.\n",
    "\n",
    "Formula: CL = Σ((1 - y_true) * d^2 + y_true * max(0, m - d)^2), where d is the distance between embeddings, y_true indicates if samples are similar or dissimilar, and m is a margin parameter.\n",
    "\n",
    "Suitable for: Siamese networks and tasks that require learning similarity or dissimilarity between samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b1ba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy Example of loss functions \n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load the dataset (assuming it's stored in a pandas DataFrame)\n",
    "import pandas as pd\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Preprocess the dataset\n",
    "# Assuming you need to convert categorical variables into numerical representations\n",
    "df = pd.get_dummies(df, columns=['Geography', 'Gender'])\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop(['RowNumber', 'CustomerId', 'Surname', 'Exited'], axis=1)\n",
    "y = df['Exited']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a simple feed-forward neural network model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model with different loss functions\n",
    "loss_functions = ['mean_squared_error', 'binary_crossentropy', 'categorical_crossentropy', 'sparse_categorical_crossentropy', 'kullback_leibler_divergence', 'huber_loss']\n",
    "for loss_func in loss_functions:\n",
    "    model.compile(optimizer='adam', loss=loss_func, metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Loss function: {loss_func}\")\n",
    "    print(f\"Test loss: {loss}\")\n",
    "    print(f\"Test accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc825f68",
   "metadata": {},
   "source": [
    "# dummy example - 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd76ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load the dataset (assuming it's stored in a pandas DataFrame)\n",
    "import pandas as pd\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Preprocess the dataset\n",
    "# Assuming you have already split the dataset into X (input features) and y (target variable)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a simple feed-forward neural network model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='linear'))  # Linear activation for regression\n",
    "\n",
    "# Compile the model with Mean Squared Error loss\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f\"Test loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913380c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb71ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d33b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b49ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58faa348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7328d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
