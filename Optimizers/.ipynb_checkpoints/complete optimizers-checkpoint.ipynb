{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d30fc1d",
   "metadata": {},
   "source": [
    "# Total Optimizers ==> \n",
    "In deep learning, optimizers are algorithms used to update the parameters of a neural network model during the training process. They play a crucial role in minimizing the loss function and improving the accuracy of the model. Here are several popular optimizers along with their explanations:\n",
    "\n",
    "(1). Stochastic Gradient Descent (SGD):\n",
    "\n",
    "SGD is one of the simplest optimizers. It updates the parameters in the direction of the negative gradient of the loss function with respect to the current mini-batch of training data.\n",
    "However, SGD has a limitation of converging slowly and can get stuck in local minima.\n",
    "\n",
    "\n",
    "(2). Momentum:\n",
    "\n",
    "The Momentum optimizer builds upon SGD by adding a momentum term. It accumulates a fraction of the previous gradients to determine the direction of the update.\n",
    "This helps to accelerate convergence and navigate through flat regions and local minima.\n",
    "\n",
    "\n",
    "(3). Nesterov Accelerated Gradient (NAG):\n",
    "\n",
    "NAG is an improvement over the Momentum optimizer. It calculates the gradient not only based on the current parameters but also using an estimate of the future parameters.\n",
    "By looking ahead, NAG allows the optimizer to better anticipate the momentum's effect and adjust its update accordingly.\n",
    "\n",
    "\n",
    "(4). AdaGrad:\n",
    "\n",
    "AdaGrad adapts the learning rate for each parameter based on the historical gradients. It increases the learning rate for infrequent features and decreases it for frequent ones.\n",
    "This makes AdaGrad well-suited for sparse data but can cause the learning rate to become too small over time.\n",
    "\n",
    "\n",
    "(5). RMSprop:\n",
    "\n",
    "RMSprop addresses the diminishing learning rate issue of AdaGrad by introducing an exponentially decaying average of squared gradients.\n",
    "By keeping a moving average of the squared gradients, RMSprop normalizes the learning rates and improves convergence.\n",
    "\n",
    "\n",
    "(6). Adam (Adaptive Moment Estimation):\n",
    "\n",
    "Adam combines the concepts of momentum and RMSprop. It maintains an exponentially decaying average of past gradients and squared gradients.\n",
    "Adam uses bias correction to account for the initialization bias in the first few iterations, making it perform well in practice across different deep learning tasks.\n",
    "\n",
    "\n",
    "(7).  AdaDelta:\n",
    "\n",
    "AdaDelta is similar to RMSprop but improves upon it by addressing the issue of the learning rate decay.\n",
    "Instead of using a global learning rate, AdaDelta uses an adaptive learning rate that is adjusted based on the moving average of gradients.\n",
    "\n",
    "\n",
    "(8). Adamax:\n",
    "\n",
    "Adamax is a variant of Adam that uses the infinity norm (maximum absolute value) of the gradient instead of the L2 norm.\n",
    "It is generally less sensitive to the scale of the gradients and is useful when dealing with sparse gradients.\n",
    "\n",
    "\n",
    "(9).  Nadam:\n",
    "\n",
    "Nadam is an extension of Adam that incorporates the Nesterov momentum into its update rule.\n",
    "By including the Nesterov momentum, Nadam aims to accelerate convergence and improve generalization.\n",
    "\n",
    "\n",
    "(10). AMSGrad:\n",
    "\n",
    "AMSGrad is a modification of Adam that prevents the learning rate from decaying too quickly.\n",
    "It maintains the maximum of the past squared gradients and uses it in the update rule, unlike Adam, which uses the exponential average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f340c5d1",
   "metadata": {},
   "source": [
    "# Advantages And Disadvantages of SGD ==> \n",
    "\n",
    "(1).  Advantages of Stochastic Gradient Descent (SGD):\n",
    "\n",
    "(a).  Simplicity: SGD is a straightforward and easy-to-implement optimization algorithm compared to more complex optimizers.\n",
    "    \n",
    "(b). Memory Efficiency: Since SGD updates the parameters using a single sample or a small batch at a time, it requires less memory compared to other optimizers that use the entire dataset.\n",
    "    \n",
    "(c). Computational Efficiency: SGD can be computationally efficient, especially for large datasets, as it processes data in small batches rather than the entire dataset at once.\n",
    "    \n",
    "(d). Generalization: SGD can help prevent overfitting by introducing randomness through the use of mini-batches, which can improve the generalization performance of the model.\n",
    "    \n",
    "(2). Disadvantages of Stochastic Gradient Descent (SGD):\n",
    "\n",
    "(a).  Noisy Updates: SGD introduces noise into the parameter updates due to the randomness of the mini-batch sampling. This noise can lead to slower convergence and instability.\n",
    "\n",
    "\n",
    "(b).  High Variance: The estimates of the gradient obtained from each mini-batch can have a high variance, which can lead to oscillations and suboptimal convergence.\n",
    "\n",
    "\n",
    "(c).  Learning Rate Selection: SGD is sensitive to the learning rate choice. Setting a learning rate that is too high can cause the optimization process to diverge, while setting it too low can result in slow convergence.\n",
    "\n",
    "\n",
    "(d).  Local Minima: SGD is susceptible to getting trapped in local minima, especially in high-dimensional and non-convex optimization problems. It may struggle to escape shallow minima and converge to the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2a9db1",
   "metadata": {},
   "source": [
    "# 2. Momentum ==> \n",
    "\n",
    "\n",
    "Momentum is an optimization algorithm that builds upon the basic Stochastic Gradient Descent (SGD) method. It helps accelerate convergence and navigate through flat regions and local minima by introducing a momentum term that keeps track of the past gradients. Here is a complete explanation of the Momentum optimizer:\n",
    "\n",
    "Algorithm Overview:\n",
    "\n",
    "In Momentum optimization, the parameter update is influenced not only by the current gradient but also by the accumulated gradient from previous iterations.\n",
    "At each iteration, the momentum optimizer computes an exponentially decaying average of the past gradients and uses this accumulated gradient to update the parameters.\n",
    "Math Formulation:\n",
    "\n",
    "The update rule for the Momentum optimizer can be expressed as follows:\n",
    "\n",
    "v(t) = m * v(t-1) + learning_rate * gradient(t)\n",
    "\n",
    "parameter(t) = parameter(t-1) - v(t)\n",
    "\n",
    "Where:\n",
    "\n",
    "v(t) is the velocity or accumulated gradient at time step t.\n",
    "m is the momentum coefficient, usually a value between 0 and 1.\n",
    "learning_rate is the learning rate or step size.\n",
    "gradient(t) is the gradient of the loss function with respect to the parameters at time step t.\n",
    "parameter(t) represents the parameters of the model at time step t.\n",
    "\n",
    "\n",
    "\n",
    "(a).  Momentum Effect:\n",
    "\n",
    "The momentum term, m * v(t-1), introduces inertia to the update process. It allows the optimizer to continue moving in the same direction as previous iterations if the gradients consistently point in that direction.\n",
    "The learning rate * gradient(t) term adjusts the update direction based on the current gradient.\n",
    "\n",
    "\n",
    "Advantages of Momentum Optimizer:\n",
    "\n",
    "(a).  Accelerated Convergence: The momentum term helps the optimizer to move faster towards the minimum by accumulating the gradients from previous iterations. This can lead to faster convergence and reduce the time needed for training.\n",
    "Smoother Optimization Trajectory: Momentum helps the optimizer smooth out the oscillations that often occur when using plain SGD, resulting in a more stable and consistent optimization trajectory.\n",
    "Improved Exploration: Momentum allows the optimizer to escape shallow local minima and navigate through flat regions, making it less likely to get stuck.\n",
    "    \n",
    "    \n",
    "(b). Hyperparameter Tuning:\n",
    "\n",
    "The momentum coefficient (m) is a hyperparameter that needs to be tuned. A value close to 1 (e.g., 0.9) is commonly used, but different problems may benefit from different values. It determines how much of the past gradients should be taken into account.\n",
    "The learning rate also needs to be properly set, considering the interaction between the momentum and learning rate hyperparameters. A high learning rate may lead to overshooting, while a low learning rate may result in slow convergence.\n",
    "\n",
    "\n",
    "(c). Variants of Momentum:\n",
    "\n",
    "Nesterov Accelerated Gradient (NAG): NAG is a modification of the momentum optimizer that calculates the gradient based on the estimated future parameters. It helps the optimizer anticipate the momentum's effect and adjust the update direction accordingly, leading to improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0eae18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\rajen\\\\OneDrive\\\\Desktop\\\\data\\\\Churn_Modelling.csv\")\n",
    "# data.head() \n",
    "\n",
    "\n",
    "# Preprocess the dataset\n",
    "X = data.iloc[:, 3:-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "# Encode categorical features\n",
    "le = LabelEncoder()\n",
    "X[:, 1] = le.fit_transform(X[:, 1])  # Geography\n",
    "X[:, 2] = le.fit_transform(X[:, 2])  # Gender\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_dim=X_train.shape[1]),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Define the loss function and metrics\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "metrics = ['accuracy']\n",
    "\n",
    "# Define the optimizer with Momentum\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c596c01f",
   "metadata": {},
   "source": [
    "# Disadvantages of Momentum ==> \n",
    "\n",
    "(1).   Overshooting:\n",
    "\n",
    "One potential issue with the Momentum optimizer is overshooting or oscillation. If the learning rate or the momentum coefficient is set too high, the optimizer may overshoot the optimal solution and oscillate around it. This can lead to slower convergence or instability in the optimization process.\n",
    "\n",
    "\n",
    "(2).  Difficulty in Converging in Some Cases:\n",
    "\n",
    "In certain scenarios, such as optimization problems with noisy or sparse gradients, the Momentum optimizer may face difficulties in converging to the optimal solution. The accumulated gradients from previous iterations can cause the optimizer to overshoot or miss important regions of the loss landscape, making convergence slower or even preventing it altogether.\n",
    "\n",
    "(3).  Sensitivity to Hyperparameters:\n",
    "\n",
    "The performance of the Momentum optimizer is highly dependent on the choice of hyperparameters, specifically the momentum coefficient and the learning rate. Finding the optimal values for these hyperparameters can be challenging and require careful tuning. Suboptimal choices can result in slow convergence or unstable behavior.\n",
    "\n",
    "(4).  Increased Memory Requirements:\n",
    "\n",
    "The Momentum optimizer requires additional memory to store and update the velocity or accumulated gradients at each iteration. While this memory overhead may not be significant for small models and datasets, it can become a concern when dealing with large-scale deep learning models or limited computational resources.\n",
    "\n",
    "(5).  Lack of Adaptivity:\n",
    "\n",
    "Momentum does not adaptively adjust its behavior based on the characteristics of the optimization problem or the current stage of training. It applies a fixed momentum coefficient throughout the training process, which may not be optimal for different stages or regions of the loss landscape. This lack of adaptivity can lead to suboptimal convergence in some cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30b0ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\rajen\\\\OneDrive\\\\Desktop\\\\data\\\\Churn_Modelling.csv\")\n",
    "# data.head() \n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess the dataset\n",
    "X = data.iloc[:, 3:-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "# Encode categorical features\n",
    "le = LabelEncoder()\n",
    "X[:, 1] = le.fit_transform(X[:, 1])  # Geography\n",
    "X[:, 2] = le.fit_transform(X[:, 2])  # Gender\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_dim=X_train.shape[1]),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Define the loss function and metrics\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "metrics = ['accuracy']\n",
    "\n",
    "# Define the optimizer with Nesterov Accelerated Gradient (NAG)\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum, nesterov=True)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb679133",
   "metadata": {},
   "source": [
    "# Disadvantages of NAG ==> \n",
    "\n",
    "(1). Sensitivity to Learning Rate: NAG can be sensitive to the choice of learning rate. If the learning rate is set too high, it may result in unstable updates and difficulty in convergence. On the other hand, setting the learning rate too low can lead to slow convergence.\n",
    "\n",
    "\n",
    "(2). Hyperparameter Tuning: Nesterov Accelerated Gradient requires tuning of the momentum coefficient and learning rate. Finding the optimal values for these hyperparameters can be a challenging task and may require extensive experimentation.\n",
    "\n",
    "\n",
    "(3). Computational Complexity: NAG involves additional computations compared to basic Momentum optimization. It requires calculating the gradient at the estimated future position of the parameters, which adds computational complexity to the optimization process. This increased computational cost can be a concern when dealing with large-scale datasets or complex models.\n",
    "\n",
    "\n",
    "(4). Limited Applicability: While NAG can be effective in many scenarios, it may not always outperform other optimization algorithms. Its performance can vary depending on the specific problem and dataset. In certain cases, alternative optimizers like Adam or RMSprop may yield better results.\n",
    "\n",
    "\n",
    "(5). Lack of Robustness to Noise: Nesterov Accelerated Gradient may exhibit reduced performance in the presence of noisy or sparse gradients. The estimation of future parameters based on the current gradient direction can be influenced by noisy updates, leading to suboptimal convergence.\n",
    "\n",
    "\n",
    "(6). Local Minima: Like other optimization algorithms, NAG is not immune to the problem of getting stuck in local minima. While it offers faster convergence in many cases, there is still a possibility of converging to suboptimal solutions depending on the nature of the loss landscape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca37ca",
   "metadata": {},
   "source": [
    "# (4). AdaGrad ==> \n",
    "\n",
    "Adagrad (Adaptive Gradient) is an optimization algorithm used in deep learning that adapts the learning rate for each parameter based on its historical gradients. It addresses the challenge of choosing a suitable learning rate by automatically adjusting it during training. \n",
    "\n",
    "(1).  Algorithm Overview:\n",
    "\n",
    "\n",
    "Adagrad adjusts the learning rate for each parameter individually based on the sum of squared gradients for that parameter.\n",
    "It assigns a larger learning rate to parameters with smaller gradients and a smaller learning rate to parameters with larger gradients.\n",
    "\n",
    "\n",
    "This adaptation of learning rates allows for more rapid progress in directions with smaller gradients and slower progress in directions with larger gradients.\n",
    "\n",
    "\n",
    "(2).  Math Formulation:\n",
    "\n",
    "The update rule for Adagrad can be expressed as follows:\n",
    "\n",
    "cache(t) = cache(t-1) + gradient(t)^2\n",
    "\n",
    "parameter(t) = parameter(t-1) - (learning_rate / sqrt(cache(t) + epsilon)) * gradient(t)\n",
    "\n",
    "Where:\n",
    "\n",
    "cache(t) is the sum of squared gradients at time step t.\n",
    "\n",
    "gradient(t) is the gradient of the loss function with respect to the parameters at time step t.\n",
    "\n",
    "parameter(t) represents the parameters of the model at time step t.\n",
    "\n",
    "learning_rate is the initial learning rate.\n",
    "\n",
    "epsilon is a small constant (e.g., 1e-8) added to the denominator for numerical stability.\n",
    "\n",
    "(3).  Accumulation of Gradients:\n",
    "\n",
    "Adagrad accumulates the squared gradients over time by summing up the square of each gradient for a specific parameter.\n",
    "This accumulation gives more weight to infrequent and large gradients, allowing the learning rate to adapt accordingly.\n",
    "\n",
    "\n",
    "(4).  Learning Rate Decay:\n",
    "\n",
    "Adagrad inherently performs learning rate decay as the accumulated squared gradients keep increasing over time.\n",
    "The learning rate becomes smaller as the cache term in the denominator becomes larger, which ensures that the learning rate decreases monotonically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4216f113",
   "metadata": {},
   "source": [
    "# Advantages of Adagrad Gradient Descent:\n",
    "\n",
    "(1).  Adaptive Learning Rates: Adagrad adapts the learning rates based on the historical gradients, enabling efficient learning for different parameters.\n",
    "\n",
    "(2).  Sparse Feature Handling: Adagrad handles sparse features well by giving them larger learning rates, making it suitable for natural language processing and recommendation systems.\n",
    "\n",
    "(3).  No Manual Learning Rate Tuning: Adagrad automatically adjusts the learning rate, reducing the need for manual tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc8800a",
   "metadata": {},
   "source": [
    "# Disadvantages of Adagrad Gradient Descent:\n",
    "\n",
    "(1).  Cumulative Gradient Squares: As Adagrad accumulates squared gradients over time, the sum of squares keeps increasing. This can result in diminishing learning rates, making the algorithm converge too slowly.\n",
    "\n",
    "\n",
    "(2).  Lack of Robustness: Adagrad might not perform well in cases where there are sudden changes in gradients or when the problem has a non-convex loss landscape.\n",
    "\n",
    "\n",
    "(3).  Difficulty with Non-Differentiable Features: Adagrad struggles with features that are non-differentiable or have a zero gradient. The accumulated gradients for such features can become too large, distorting the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d6feb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\rajen\\\\OneDrive\\\\Desktop\\\\data\\\\Churn_Modelling.csv\")\n",
    "# data.head() \n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess the dataset\n",
    "X = data.iloc[:, 3:-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "# Encode categorical features\n",
    "le = LabelEncoder()\n",
    "X[:, 1] = le.fit_transform(X[:, 1])  # Geography\n",
    "X[:, 2] = le.fit_transform(X[:, 2])  # Gender\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_dim=X_train.shape[1]),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Define the loss function and metrics\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "metrics = ['accuracy']\n",
    "\n",
    "# Define the optimizer with Adagrad\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.keras.optimizers.Adagrad(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e988a8b5",
   "metadata": {},
   "source": [
    "# (5). RMSprop ==> \n",
    "\n",
    "RMSprop (Root Mean Square Propagation) is an optimization algorithm commonly used in deep learning. It addresses the limitations of the Adagrad optimizer by introducing a moving average of squared gradients. Here's a complete explanation of RMSprop:\n",
    "\n",
    "(1).  Algorithm Overview:\n",
    "\n",
    "RMSprop adapts the learning rate for each parameter based on the moving average of squared gradients.\n",
    "It maintains a weighted average of the squared gradients over time, which helps control the learning rate.\n",
    "Math Formulation:\n",
    "\n",
    "The update rule for RMSprop can be expressed as follows:\n",
    "\n",
    "cache(t) = decay_rate * cache(t-1) + (1 - decay_rate) * gradient(t)^2\n",
    "\n",
    "parameter(t) = parameter(t-1) - (learning_rate / sqrt(cache(t) + epsilon)) * gradient(t)\n",
    "\n",
    "Where:\n",
    "\n",
    "cache(t) is the moving average of squared gradients at time step t.\n",
    "gradient(t) is the gradient of the loss function with respect to the parameters at time step t.\n",
    "parameter(t) represents the parameters of the model at time step t.\n",
    "learning_rate is the initial learning rate.\n",
    "decay_rate is a hyperparameter that controls the weightage of previous squared gradients.\n",
    "epsilon is a small constant (e.g., 1e-8) added to the denominator for numerical stability.\n",
    "\n",
    "\n",
    "(2).  Accumulation of Squared Gradients:\n",
    "\n",
    "RMSprop accumulates the squared gradients by taking a weighted average of the squared gradients from previous time steps.\n",
    "The decay_rate hyperparameter controls the rate at which previous gradients contribute to the current cache.\n",
    "\n",
    "\n",
    "(3).  Learning Rate Adaptation:\n",
    "\n",
    "RMSprop adapts the learning rate for each parameter based on the square root of the average squared gradients.\n",
    "It divides the learning rate by the square root of the cache value, which effectively scales the learning rate based on the magnitude of the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853af6b4",
   "metadata": {},
   "source": [
    "# Advantages of RMSprop:\n",
    "\n",
    "(1). Adaptive Learning Rates: RMSprop adapts the learning rates based on the average squared gradients, allowing for efficient learning across different parameters.\n",
    "    \n",
    "    \n",
    "(2). Stable Convergence: By maintaining a moving average of squared gradients, RMSprop can handle noisy or sparse gradients more robustly, leading to more stable convergence.\n",
    "    \n",
    "    \n",
    "(3). Reduced Sensitivity to Learning Rate: RMSprop's adaptation of the learning rate reduces the need for manual tuning, making it less sensitive to the choice of the initial learning rate.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b8d5ac",
   "metadata": {},
   "source": [
    "# Disadvantages of RMSprop:\n",
    "\n",
    "(1).  Difficulty with Non-Stationary Problems: RMSprop may not perform well on non-stationary problems where the optimal learning rate changes over time. The accumulated squared gradients can cause the learning rate to become too small.\n",
    "\n",
    "\n",
    "(2).  Limited Memory: RMSprop relies on a fixed-size cache to store the average squared gradients. In some cases, the limited memory may hinder the algorithm's ability to adapt to the dynamics of the gradients effectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98e447c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\rajen\\\\OneDrive\\\\Desktop\\\\data\\\\Churn_Modelling.csv\")\n",
    "# data.head() \n",
    "\n",
    "# Preprocess the dataset\n",
    "X = data.iloc[:, 3:-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "# Encode categorical features\n",
    "le = LabelEncoder()\n",
    "X[:, 1] = le.fit_transform(X[:, 1])  # Geography\n",
    "X[:, 2] = le.fit_transform(X[:, 2])  # Gender\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_dim=X_train.shape[1]),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Define the loss function and metrics\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "metrics = ['accuracy']\n",
    "\n",
    "# Define the optimizer with RMSprop\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a170b71f",
   "metadata": {},
   "source": [
    "# Adam ==> \n",
    "\n",
    "The Adam optimizer is a popular optimization algorithm commonly used in deep learning and machine learning. It stands for \"Adaptive Moment Estimation\" and combines the concepts of momentum and adaptive learning rates.\n",
    "\n",
    "Here's a complete overview of the Adam optimizer:\n",
    "\n",
    "Background:\n",
    "The Adam optimizer was introduced in 2014 by Diederik P. Kingma and Jimmy Ba. It was designed to address some limitations of other optimization algorithms like stochastic gradient descent (SGD) and its variants.\n",
    "\n",
    "Algorithm:\n",
    "The Adam optimizer maintains a set of adaptive learning rates for individual parameters. It computes individual adaptive learning rates based on the estimate of both the first-order (mean of gradients) and second-order (uncentered variance of gradients) moments of the gradients.\n",
    "\n",
    "The algorithm involves the following steps:\n",
    "\n",
    "a. Initialization:\n",
    "\n",
    "Initialize the first-moment variable (mean of gradients) to zero.\n",
    "\n",
    "Initialize the second-moment variable (variance of gradients) to zero.\n",
    "\n",
    "Set the hyperparameters: learning rate (α), decay rates for the moment estimates (β1 and β2), and a small constant for numerical stability (ε).\n",
    "\n",
    "b. Update at each iteration:\n",
    "\n",
    "Compute the gradient of the objective function with respect to the parameters.\n",
    "\n",
    "\n",
    "Update the first-moment variable:\n",
    "\n",
    "Multiply the first-moment variable by β1.\n",
    "\n",
    "Add (1 - β1) times the gradient to the first-moment variable.\n",
    "\n",
    "Update the second-moment variable:\n",
    "\n",
    "Multiply the second-moment variable by β2.\n",
    "\n",
    "Add (1 - β2) times the squared gradient to the second-moment variable.\n",
    "\n",
    "Correct for bias:\n",
    "\n",
    "Compute the bias-corrected first-moment estimate.\n",
    "\n",
    "Compute the bias-corrected second-moment estimate.\n",
    "\n",
    "Update the parameters:\n",
    "\n",
    "Update the parameters by subtracting the learning rate multiplied by the first-moment estimate divided by the square root of the second-moment estimate (plus ε for numerical stability).\n",
    "\n",
    "The algorithm adapts the learning rates for each parameter based on the gradient's statistics, allowing it to converge faster and handle sparse gradients efficiently.\n",
    "\n",
    "\n",
    "Hyperparameters:\n",
    "The Adam optimizer requires setting several hyperparameters:\n",
    "\n",
    "(1). Learning rate (α): Determines the step size taken during parameter updates.\n",
    "\n",
    "(2). Decay rate for the first-moment estimate (β1): Controls the exponential decay of the moving average of gradients.\n",
    "\n",
    "(3). Decay rate for the second-moment estimate (β2): Controls the exponential decay of the moving average of squared gradients.\n",
    "\n",
    "(4). Small constant (ε): Added for numerical stability to prevent division by zero.\n",
    "\n",
    "The values of these hyperparameters can significantly affect the performance of the optimizer and need to be tuned based on the specific problem and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def61516",
   "metadata": {},
   "source": [
    "# Advantages of Adam ==> \n",
    "\n",
    "(1).  Efficient: Adam maintains per-parameter learning rates, adapting them individually based on the estimated moments of the gradients.\n",
    "    \n",
    "(2).  Robust to sparse gradients: It performs well even when dealing with sparse gradients, making it suitable for tasks like natural language processing (NLP).\n",
    "    \n",
    "(3).  Converges faster: Adam's adaptive learning rates allow it to converge faster compared to traditional optimization algorithms.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eece1e05",
   "metadata": {},
   "source": [
    "# Disadvantages of Adam ==> \n",
    "\n",
    "(1). Increased memory usage: Adam maintains additional variables for the adaptive learning rates, leading to higher memory requirements compared to simpler optimizers like SGD.\n",
    "    \n",
    "    \n",
    "(2). Sensitivity to hyperparameters: The performance of Adam can be sensitive to the choice of hyperparameters, and improper tuning may result in suboptimal convergence.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49799035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\rajen\\\\OneDrive\\\\Desktop\\\\data\\\\Churn_Modelling.csv\")\n",
    "# data.head() \n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess the categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "data['Geography'] = label_encoder.fit_transform(data['Geography'])\n",
    "data['Gender'] = label_encoder.fit_transform(data['Gender'])\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = data.drop(['Exited', 'RowNumber', 'CustomerId', 'Surname'], axis=1)\n",
    "y = data['Exited']\n",
    "\n",
    "# Normalize the numerical features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e398e228",
   "metadata": {},
   "source": [
    "# AdaDelta ==> \n",
    "\n",
    "AdaDelta is an optimization algorithm used in deep learning and machine learning to update the parameters of a neural network. It is a variant of the stochastic gradient descent (SGD) optimization method that aims to address some of the limitations of traditional SGD, such as the sensitivity to learning rate tuning.\n",
    "\n",
    "Complete overview of the AdaDelta optimizer:\n",
    "\n",
    "(1). Background:\n",
    "AdaDelta was proposed by Matthew D. Zeiler in 2012 as an extension of the Adagrad optimizer. Adagrad uses a per-parameter learning rate that adapts over time, but it suffers from a problem known as \"learning rate decay,\" which causes the learning rates to become very small, making further learning difficult. AdaDelta was designed to overcome this issue and provide a more stable and effective optimization method.\n",
    "\n",
    "(2). Adaptive Learning Rate:\n",
    "Like Adagrad, AdaDelta adapts the learning rate for each parameter based on the magnitudes of its gradients. However, instead of accumulating the squared gradients over time, AdaDelta uses a more sophisticated scheme that restricts the accumulated sum of gradients to a fixed-size sliding window.\n",
    "\n",
    "(3). Accumulated Gradient:\n",
    "AdaDelta keeps track of the accumulated sum of squared gradients, denoted by E[g^2], for each parameter. It is computed using an exponential moving average formula, where the accumulated gradient is decayed and updated with each iteration.\n",
    "\n",
    "(4). Root Mean Square Update (RMS):\n",
    "In addition to the accumulated gradient, AdaDelta maintains an additional parameter called the root mean square update, denoted by E[dx^2]. Similar to the accumulated gradient, this value is computed using an exponential moving average. It represents the magnitude of the updates to the parameters.\n",
    "\n",
    "(5). Parameter Update:\n",
    "The update for each parameter is calculated using the following steps:\n",
    "    \n",
    "    \n",
    "(a).   Compute the root mean square of the accumulated gradient:\n",
    "    \n",
    "    \n",
    "RMS[g] = sqrt(E[g^2] + epsilon)\n",
    "Here, epsilon is a small constant (e.g., 1e-8) added for numerical stability.\n",
    "\n",
    "\n",
    "(b).    Compute the root mean square of the previous parameter updates:\n",
    "RMS[dx] = sqrt(E[dx^2] + epsilon)\n",
    "(c). Compute the update step for each parameter:\n",
    "    \n",
    "    \n",
    "dx = -(RMS[dx] / RMS[g]) * g\n",
    "d. Update the accumulated gradient:\n",
    "E[g^2] = rho * E[g^2] + (1 - rho) * g^2\n",
    "Here, rho is a decay rate hyperparameter (e.g., 0.9) that controls the weighting of the current gradient compared to the accumulated gradient.\n",
    "\n",
    "\n",
    "(e).    Update the root mean square of the parameter updates:\n",
    "E[dx^2] = rho * E[dx^2] + (1 - rho) * dx^2\n",
    "\n",
    "\n",
    "(f). Update the parameters:\n",
    "    \n",
    "    \n",
    "param += dx\n",
    "\n",
    "(6). Learning Rate Adaptation:\n",
    "    \n",
    "    \n",
    "AdaDelta eliminates the need for manually setting a learning rate. It adapts the learning rate based on the historical gradients and updates of the parameters. The magnitudes of the gradients and updates influence the effective learning rate for each parameter. AdaDelta allows for more robust learning and reduces the need for fine-tuning the learning rate hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46210267",
   "metadata": {},
   "source": [
    "# Benefits of AdaDelta:\n",
    "\n",
    "(1).  AdaDelta can handle sparse gradients well, making it suitable for problems with large-scale datasets and high-dimensional spaces.\n",
    "\n",
    "(2).  It eliminates the need for setting a global learning rate manually, reducing the sensitivity to learning rate tuning.\n",
    "\n",
    "\n",
    "(3).  AdaDelta's adaptive learning rate allows for smoother convergence during training and helps prevent overshooting or getting stuck in plateaus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99906b17",
   "metadata": {},
   "source": [
    "# Limitations of AdaDelta:\n",
    "\n",
    "(1).  AdaDelta has some additional hyperparameters to tune, such as the decay rate (rho) and a small constant (epsilon), although they are generally less sensitive than the learning rate in traditional optimization methods.\n",
    "\n",
    "(2).  It may require more memory to store the additional parameters (E[g^2] and E[dx^2]) for each parameter compared to standard optimization methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d02c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\rajen\\\\OneDrive\\\\Desktop\\\\data\\\\Churn_Modelling.csv\")\n",
    "# df.head() \n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "# Assuming you have already loaded your dataset into a pandas DataFrame called 'df'\n",
    "\n",
    "# Perform label encoding for categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "df['Geography'] = label_encoder.fit_transform(df['Geography'])\n",
    "df['Gender'] = label_encoder.fit_transform(df['Gender'])\n",
    "\n",
    "# Split the dataset into features and target variable\n",
    "X = df.drop(['Exited', 'Surname', 'RowNumber'], axis=1).values\n",
    "y = df['Exited'].values\n",
    "\n",
    "# Perform feature scaling\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(units=64, activation='relu', input_dim=X_train.shape[1]),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adadelta(),  # Use AdaDelta optimizer\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566a665d",
   "metadata": {},
   "source": [
    "# Adamax Optimizer ===> \n",
    "\n",
    "\n",
    "Adamax is an optimization algorithm that is commonly used in deep learning models. It is a variant of the Adam (Adaptive Moment Estimation) optimizer, which combines the benefits of both the adaptive learning rate method and the moving average of gradients. Adamax extends Adam by using the infinity norm (max norm) for the update step instead of the L2 norm used in Adam.\n",
    "\n",
    "The key components and equations used in the Adamax optimizer:\n",
    "\n",
    "(1).  Initialization:\n",
    "\n",
    "(a).Initialize the parameters:\n",
    "    \n",
    "Learning rate (α): Typically a small value, e.g., 0.001.\n",
    "    \n",
    "    \n",
    "β1: Exponential decay rate for the first moment estimates. Commonly set to 0.9.\n",
    "    \n",
    "    \n",
    "β2: Exponential decay rate for the second moment estimates. Commonly set to 0.999.\n",
    "    \n",
    "    \n",
    "ε: A small value added to the denominator to avoid division by zero. Typically around 1e-8.\n",
    "    \n",
    "    \n",
    "Initialize the first moment vector (m) and the exponentially weighted infinity norm vector (u) for each parameter to zero.\n",
    "\n",
    "\n",
    "(2).  Iterative update:\n",
    "    \n",
    "(a). For each iteration/timestep t:\n",
    "\n",
    "Compute the gradients of the loss function with respect to the parameters.\n",
    "\n",
    "(b). Update the first moment estimates (m) using the exponential decay rate β1:\n",
    "    \n",
    "m = β1 * m + (1 - β1) * gradients\n",
    "\n",
    "(c). Update the second moment estimates (v) using the exponential decay rate β2:\n",
    "    \n",
    "u = max(β2 * u, abs(gradients))\n",
    "\n",
    "(d). Compute the bias-corrected first moment estimate:\n",
    "    \n",
    "m_hat = m / (1 - β1^t)\n",
    "\n",
    "\n",
    "(e). Update the parameters using the Adamax update rule:\n",
    "    \n",
    "parameter = parameter - (α / (1 - β1^t)) * (m_hat / (u + ε))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a77754",
   "metadata": {},
   "source": [
    "# Summary ==> \n",
    "the Adamax optimizer uses the first and second moment estimates (m and u, respectively) to adaptively update the parameters. The first moment estimate represents the average of past gradients, and the second moment estimate represents the exponentially weighted infinity norm of past gradients. By using the infinity norm, Adamax provides a more stable update step when the gradients have varying magnitudes.\n",
    "\n",
    "Adamax is known for its robustness and ability to handle sparse gradients effectively. It has been widely used in various deep learning architectures and has shown good performance in many applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a05bb1",
   "metadata": {},
   "source": [
    "# Advantages of Adamax optimizer ==> \n",
    "\n",
    "(1). Adaptive learning rate: Adamax adjusts the learning rate for each parameter individually based on the magnitudes of the past gradients. This adaptive behavior allows it to handle different learning rates for different parameters, leading to efficient optimization.\n",
    "    \n",
    "\n",
    "(2). Momentum-based updates: Adamax incorporates momentum through the first moment estimates (m), which helps accelerate convergence and smooth out noisy gradients. The momentum term allows the optimizer to continue moving in the right direction even when the gradients fluctuate.\n",
    "    \n",
    "\n",
    "(3). Robustness to sparse gradients: Adamax performs well in scenarios where the gradients are sparse, meaning only a few parameters have significant updates. It adapts to different magnitudes of gradients by using the infinity norm, which can handle large gradients effectively.\n",
    "    \n",
    "\n",
    "(4). Low memory requirements: Adamax requires minimal memory to store the first moment estimates (m) and the exponentially weighted infinity norm vector (u). These vectors have the same dimensionality as the model parameters, making Adamax memory-efficient compared to some other optimization algorithms.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871130d3",
   "metadata": {},
   "source": [
    "# Disadvantages of Adamax optimizer ==> \n",
    "\n",
    "(1). Sensitivity to learning rate: Adamax can be sensitive to the choice of the initial learning rate. Setting a learning rate that is too high may result in unstable updates or failure to converge, while setting it too low can lead to slow convergence.\n",
    "    \n",
    "\n",
    "(2). Potential for overshooting: The momentum-based updates in Adamax may cause the optimizer to overshoot the minimum, especially when the gradients are noisy or the learning rate is high. This behavior can result in slower convergence or oscillations around the optimal solution.\n",
    "    \n",
    "\n",
    "(3). Lack of theoretical guarantees: Unlike some other optimization algorithms, such as stochastic gradient descent with momentum, Adamax does not have strong theoretical convergence guarantees. While it often performs well in practice, its convergence properties are not yet well-understood.\n",
    "    \n",
    "\n",
    "(4). Additional hyperparameters: Adamax introduces additional hyperparameters, such as the exponential decay rates (β1 and β2) and the small constant (ε) added to the denominator. Choosing appropriate values for these hyperparameters may require some trial and error or hyperparameter tuning.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cc91ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\rajen\\\\OneDrive\\\\Desktop\\\\data\\\\Churn_Modelling.csv\")\n",
    "# data.head() \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers\n",
    "\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop(columns = ['Surname' , 'Exited'], axis=1)\n",
    "y = data['Exited']\n",
    "\n",
    "# Perform label encoding for categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "X['Geography'] = label_encoder.fit_transform(X['Geography'])\n",
    "X['Gender'] = label_encoder.fit_transform(X['Gender'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the model architecture\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with Adamax optimizer\n",
    "optimizer = optimizers.Adamax(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad32996",
   "metadata": {},
   "source": [
    "# Nadam Optimizer ===> \n",
    "\n",
    "Nadam (Nesterov-accelerated Adaptive Moment Estimation) is an optimization algorithm that combines the advantages of Nesterov accelerated gradient (NAG) and Adam (Adaptive Moment Estimation). It is a variant of Adam that incorporates Nesterov momentum into its update rule. Nadam aims to provide faster convergence and better generalization performance compared to traditional momentum-based optimizers.\n",
    "\n",
    "The key components and equations used in the Nadam optimizer:\n",
    "\n",
    "(1). Initialization:\n",
    "\n",
    "Initialize the parameters:\n",
    "Learning rate (α): Typically a small value, e.g., 0.001.\n",
    "β1: Exponential decay rate for the first moment estimates. Commonly set to 0.9.\n",
    "β2: Exponential decay rate for the second moment estimates. Commonly set to 0.999.\n",
    "ε: A small value added to the denominator to avoid division by zero. Typically around 1e-8.\n",
    "Initialize the first moment vector (m) and the second moment vector (v) for each parameter to zero.\n",
    "\n",
    "\n",
    "(2).  Iterative update:\n",
    "\n",
    "For each iteration/timestep t:\n",
    "\n",
    "Compute the gradients of the loss function with respect to the parameters.\n",
    "\n",
    "Update the first moment estimates (m) using the exponential decay rate β1:\n",
    "\n",
    "m = β1 * m + (1 - β1) * gradients\n",
    "\n",
    "Update the second moment estimates (v) using the exponential decay rate β2:\n",
    "\n",
    "v = β2 * v + (1 - β2) * gradients^2\n",
    "\n",
    "\n",
    "Compute the bias-corrected first and second moment estimates:\n",
    "\n",
    "m_hat = m / (1 - β1^t)\n",
    "v_hat = v / (1 - β2^t)\n",
    "\n",
    "Compute the Nesterov momentum update:\n",
    "momentum = β1 * m_hat + ((1 - β1) * gradients) / (1 - β1^t)\n",
    "Update the parameters using the Nadam update rule:\n",
    "parameter = parameter - (α / (sqrt(v_hat) + ε)) * (momentum + (β1 * momentum) - ((1 - β1) * gradients) / (1 - β1^t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300b96e1",
   "metadata": {},
   "source": [
    "# Short summary ==> \n",
    "Nadam combines the benefits of Nesterov accelerated gradient, which improves convergence near the minimum, with the adaptive learning rate and momentum features of Adam. By incorporating Nesterov momentum, Nadam has shown improved performance on deep learning models, particularly for tasks involving large-scale and complex datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eebcc3",
   "metadata": {},
   "source": [
    "# Advantages of Nadam Optimizers ==> \n",
    "\n",
    "(1).  Fast convergence: Nadam combines the benefits of Nesterov accelerated gradient (NAG) and Adam optimizer, resulting in faster convergence compared to traditional momentum-based optimizers. The inclusion of Nesterov momentum allows the optimizer to make more accurate updates and handle complex optimization landscapes.\n",
    "\n",
    "(2).  Good generalization performance: Nadam has been observed to have good generalization performance, meaning it can effectively generalize to unseen data beyond the training set. This can lead to improved accuracy and robustness of deep learning models.\n",
    "\n",
    "(3).  Adaptive learning rate: Similar to Adam, Nadam adjusts the learning rate for each parameter individually based on the magnitudes of the first and second moment estimates. This adaptiveness enables efficient optimization and helps overcome the challenges of using a fixed learning rate.\n",
    "\n",
    "(4). Robustness to noisy gradients: The combination of Nesterov momentum and adaptive learning rate in Nadam helps handle noisy gradients effectively. The optimizer can navigate through noisy or sparse gradients, resulting in stable and reliable updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6e2830",
   "metadata": {},
   "source": [
    "# Disadvantages of Nadam Optimizers ==> \n",
    "\n",
    "(1). Hyperparameter sensitivity: Nadam, like other optimization algorithms, requires careful tuning of hyperparameters to achieve optimal performance. The choice of learning rate, decay rates, and other hyperparameters can impact the convergence and generalization ability of the model. It may require some experimentation and tuning to find the best set of hyperparameters.\n",
    "    \n",
    "\n",
    "(2). Computational overhead: Nadam involves additional computations compared to simpler optimization algorithms. It requires calculating and maintaining the first and second moment estimates for each parameter, which can increase the computational overhead. While this additional complexity is generally acceptable in most scenarios, it might be a consideration for computationally constrained environments.\n",
    "    \n",
    "\n",
    "(3). Lack of theoretical guarantees: Although Nadam has shown promising performance in practice, it lacks strong theoretical convergence guarantees like some other optimization algorithms. Its convergence properties are still an active area of research, and the behavior of Nadam in different optimization scenarios is not yet fully understood.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fcef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\rajen\\\\OneDrive\\\\Desktop\\\\data\\\\Churn_Modelling.csv\")\n",
    "# data.head() \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers\n",
    "\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop(columns = ['Surname','Exited'], axis=1)\n",
    "y = data['Exited']\n",
    "\n",
    "# Perform label encoding for categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "X['Geography'] = label_encoder.fit_transform(X['Geography'])\n",
    "X['Gender'] = label_encoder.fit_transform(X['Gender'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the model architecture\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with Nadam optimizer\n",
    "optimizer = optimizers.Nadam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786a1188",
   "metadata": {},
   "source": [
    "# AMSGrad   Optimizer ==> \n",
    "\n",
    "AMSGrad (Adaptive Moment Estimation for Stochastic Gradient Descent) is an optimization algorithm that extends the Adam (Adaptive Moment Estimation) optimizer. It aims to address a limitation of Adam where the accumulated second moment estimates (v) can grow indefinitely, potentially leading to suboptimal convergence or oscillations in certain scenarios. AMSGrad modifies the update rule of Adam to ensure the boundedness of the second moment estimates.\n",
    "\n",
    "The key components and equations used in the AMSGrad optimizer:\n",
    "\n",
    "(1).  Initialization:\n",
    "\n",
    "(a). Initialize the parameters:\n",
    "    \n",
    "Learning rate (α): Typically a small value, e.g., 0.001.\n",
    "    \n",
    "(b). β1: Exponential decay rate for the first moment estimates. Commonly set to 0.9.\n",
    "    \n",
    "β2: Exponential decay rate for the second moment estimates. Commonly set to 0.999.\n",
    "    \n",
    "ε: A small value added to the denominator to avoid division by zero. Typically around 1e-8.\n",
    "    \n",
    "Initialize the first moment vector (m) and the maximum second moment vector (v_max) for each parameter to zero.\n",
    "\n",
    "\n",
    "\n",
    "(2). Iterative update:\n",
    "For each iteration/timestep t:\n",
    "\n",
    "Compute the gradients of the loss function with respect to the parameters.\n",
    "\n",
    "Update the first moment estimates (m) using the exponential decay rate β1:\n",
    "    \n",
    "m = β1 * m + (1 - β1) * gradients\n",
    "\n",
    "Update the second moment estimates (v) using the exponential decay rate β2:\n",
    "    \n",
    "v = β2 * v + (1 - β2) * gradients^2\n",
    "\n",
    "Update the maximum second moment estimates (v_max) as the maximum element-wise value between the current v_max and v:\n",
    "    \n",
    "v_max = max(v_max, v)\n",
    "\n",
    "Compute the bias-corrected first and second moment estimates:\n",
    "    \n",
    "m_hat = m / (1 - β1^t)\n",
    "\n",
    "v_hat = v_max / (1 - β2^t)\n",
    "\n",
    "Update the parameters using the AMSGrad update rule:\n",
    "    \n",
    "parameter = parameter - (α / (sqrt(v_hat) + ε)) * m_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6775d7",
   "metadata": {},
   "source": [
    "# In summary, AMSGrad introduces the v_max term to ensure that the second moment estimates do not grow indefinitely. By using the maximum second moment estimates, AMSGrad guarantees that the update direction remains stable, preventing the oscillatory behavior that can occur in Adam when v grows rapidly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadbd06b",
   "metadata": {},
   "source": [
    "# Advantages of AMSGrad optimizer ==> \n",
    "\n",
    "(1). Stability and avoidance of oscillations: The boundedness of the second moment estimates in AMSGrad helps prevent the occurrence of oscillations and improves stability during optimization. This stability is particularly beneficial in scenarios with non-convex and ill-conditioned optimization landscapes.\n",
    "    \n",
    "\n",
    "(2). Improved convergence in certain cases: AMSGrad has demonstrated improved convergence properties compared to Adam in scenarios where the Adam optimizer fails to converge due to unbounded second moment estimates. AMSGrad can provide more reliable and efficient optimization in such cases.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fab55fc",
   "metadata": {},
   "source": [
    "# Disadvantages of AMSGrad optimizer ==> \n",
    "\n",
    "(1). Potentially slower convergence: In some cases, the modifications made in AMSGrad to ensure boundedness of second moment estimates may lead to slower convergence compared to Adam. While AMSGrad addresses the issue of unboundedness, it may sacrifice some of the adaptive learning rate properties of Adam.\n",
    "    \n",
    "\n",
    "(2). Additional computational overhead: The additional computations involved in AMSGrad, such as calculating the maximum second moment estimates (v_max), introduce some computational overhead compared to simpler optimization algorithms. However, this additional complexity is generally acceptable in most scenarios.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
